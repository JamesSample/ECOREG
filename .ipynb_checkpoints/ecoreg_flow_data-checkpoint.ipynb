{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt, seaborn as sn, mpld3\n",
    "import pandas as pd, os, glob\n",
    "sn.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECOREG\n",
    "## Discharge data processing\n",
    "\n",
    "The notebook [here](http://nbviewer.jupyter.org/github/JamesSample/ECOREG/blob/master/ecoreg_data_exploration.ipynb) documents my initial exploration of the German discharge data for the ECOREG project. In a meeting on 02/05/2016, it was decided that, initially, Jannicke and Susi will collate and process the ecology data while I calculate IHA parameters for all the German and Norwegian flow sites (see Susi's e-mail from 03/05/2016 at 10:20 for more details). \n",
    "\n",
    "This notebook describes the tidying and processing of the discharge data. Note that all the relevant raw data files can be found here:\n",
    "\n",
    "K:\\Prosjekter\\Ferskvann\\O-13026 ECOREG\n",
    "\n",
    "I've have also created slightly modified versions of these files on my local drive. These are used as inputs to the code below.\n",
    "\n",
    "## 1. Sampling times for ecology\n",
    "\n",
    "For consistency, wherever possible we would like to calculate the IHA parameters based on **5 years** of discharge data prior to the ecological sampling. The IHA methodology generally uses **water years** running from the **start of October to the end of September**, and the water chemistry and ecological surveys in Norway were all undertaken during **September 2013** (see the *Field data* sheet of *ECOREG_WP1_macroinvertebrate data.xlsx* for details). For Norway, it therefore makes sense to calculate the IHA parameters using discharge data for the five year period from **01/10/2008 to 30/09/2013 (water years 2009 to 2013 inclusive)**.\n",
    "\n",
    "The German sites are a little more complicated. The table below shows the various sampling dates for water chemistry and ecology.\n",
    "\n",
    "**NB:** I'm guessing that *PB* in the table below relates to benthic algae and *MZB* is the macroinvertebrate data? **Check with Susi**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '\\\\\\\\niva-of5\\\\osl-userdata$\\\\JES\\\\Documents\\\\James_Work\\\\Staff\\\\Susi_S\\\\ECOREG\\\\Raw_Data\\\\Germany\\\\sites_and_ecol.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-637cadee2023>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mger_eco_xls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'\\\\niva-of5\\osl-userdata$\\JES\\Documents\\James_Work\\Staff\\Susi_S\\ECOREG\\Raw_Data\\Germany\\sites_and_ecol.xlsx'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mger_eco_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mger_eco_xls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msheetname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' overview (detail)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Get columns of interest and rename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Data\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\pandas\\io\\excel.pyc\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheetname, header, skiprows, skip_footer, index_col, names, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, true_values, false_values, engine, squeeze, **kwds)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     return io._parse_excel(\n",
      "\u001b[1;32mC:\\Data\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\pandas\\io\\excel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, **kwds)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[1;32mC:\\Data\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\xlrd\\__init__.pyc\u001b[0m in \u001b[0;36mopen_workbook\u001b[1;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '\\\\\\\\niva-of5\\\\osl-userdata$\\\\JES\\\\Documents\\\\James_Work\\\\Staff\\\\Susi_S\\\\ECOREG\\\\Raw_Data\\\\Germany\\\\sites_and_ecol.xlsx'"
     ]
    }
   ],
   "source": [
    "# Get sampling/survey times for German sites\n",
    "\n",
    "ger_eco_xls = r'\\\\niva-of5\\osl-userdata$\\JES\\Documents\\James_Work\\Staff\\Susi_S\\ECOREG\\Raw_Data\\Germany\\sites_and_ecol.xlsx'\n",
    "ger_eco_df = pd.read_excel(ger_eco_xls, sheetname=' overview (detail)')\n",
    "\n",
    "# Get columns of interest and rename\n",
    "df = ger_eco_df[['Gauge', 'date PB', 'date chemistry', 'date MZB']].copy()\n",
    "df.columns = ['Gauge', 'PB', 'Chem', 'MZB']\n",
    "\n",
    "# Sort values according to site and date of benthic algae survey\n",
    "df.sort_values(['Gauge', 'PB'], ascending=True, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from this table that surveys at the German sites often take place at different times. In particular, the MZB surveys are frequently several years apart from the closest PB and chemistry sampling data. **Susi and Jannicke will need to decide exactly which data to use here**. In general, the PB and chemistry data seem to be more consistent: unlike MZB, these columns have no missing values and the water sampling and ecological surveys typically take place within the same month. I'm therefore tempted to use these columns to define the IHA discharge time periods.\n",
    "\n",
    "The above table also shows that some of the sites have been surveyed more than once. To be compatible with the Norwegian data, it makes sense to use data **collected during the autumn** wherever possible. In Excel, I have added columns identifying which surveys look best for each site, as well as giving the preceeding 5 year time period for the discharge data (see *sites_and_ecol.xlsx* on my local drive for details). I have also added a quality control (*QC*) column, which highlights when the only available surveys are widely separated in time. **This all needs checking with Susi and Jannicke**.\n",
    "\n",
    "Another issue to consider is whether we want use multiple datasets for the same site? For example, the German station **Bueren** has two plausible datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dates for Bueren\n",
    "df[df['Gauge']=='Bueren']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first of these may be the best to use, as the three surveys are closer together in time. The second one could be OK as well, though, and the more data we have in the analysis the better. The main problem with incorporating both is that the overlapping time periods for the flow indicators mean the IHA scores would not be strictly **independent**, which could bias our statistical results. In practice I suspect this effect will be small, but for the moment I've chosen **just one dataset per site** to keep everything \"balanced\". We can change this later if we want to.\n",
    "\n",
    "Note that in some cases the choice of which survey dataset to use is not obvious. For example, **Herrntrop**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dates for Herrntrop\n",
    "df[df['Gauge']=='Herrntrop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we can either have all three surveys taken during the same year, but sampled during the spring and summer, or we can have PB and chemistry samples from the autumn (which is ideal), but an MZB survey that is then more than 7 years \"out of date\". I'm not sure which is the better option here, but for now I've chosen to **prioritise surveys taken in the same year** (i.e. use the first row from the table above, rather than the second). **Check with Susi and Jannicke**.\n",
    "\n",
    "Having done this, a summary for the German sites looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get columns of interest and rename\n",
    "cols = ['Gauge', 'date PB', 'date chemistry', 'date MZB',\n",
    "        'Use_Data', 'St_Yr_Oct', 'End_Yr_Sept', 'QC']\n",
    "df = ger_eco_df[cols].copy()\n",
    "df.columns = ['Gauge', 'PB', 'Chem', 'MZB', 'Use', 'St', 'End', 'QC']\n",
    "\n",
    "# Extract columns where Use = 1\n",
    "df = df[df['Use']==1]\n",
    "\n",
    "# Reformat data for ease of reading\n",
    "for col in ['PB', 'Chem', 'MZB']:\n",
    "    df[col] = df[col].apply(lambda x: x.strftime(format='%m-%Y'))\n",
    "    \n",
    "# Sort values according to site and date of benthic algae survey\n",
    "df.sort_values(['Gauge', 'PB'], ascending=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print 'Total number of German sites with chemistry and ecology data: %s' % len(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table has a single entry for each of the 31 German sites. However, the MZB surveys for **Bamenohl**, **Boerlinghausen** and **Bredelar** are very old compared to the PB and chemistry data - so old, in fact, that the MZB surveys predate the 5 year window under consideration for the discharge parameters. In addition, the flow regime at Bamenohl is unusual (see the [previous notebook](http://nbviewer.jupyter.org/github/JamesSample/ECOREG/blob/master/ecoreg_data_exploration.ipynb) for more details). It might therefore be a good idea to **remove these sites from further consideration**. This is done below, but can be easily changed again later if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sites to remove\n",
    "drop_sites = ['Bamenohl', 'Boerlinghausen', 'Bredelar']\n",
    "\n",
    "# Remove\n",
    "df = df.query('Gauge not in @drop_sites')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Update ger_eco_df so we can use this later\n",
    "ger_eco_df = df\n",
    "ger_eco_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Availability of discharge data\n",
    "\n",
    "The next step is to check whether discharge data are available for the 5 year period of interest at each site. Susi has already confirmed that the monitoring records are complete for all the Norwegian sites, except for **Homstølvatn ndf** (NVE site code 25.6), which only has four years of data (the entire 2009 calendar year is missing). The German data will need more careful checking.\n",
    "\n",
    "### 2.1. Norwegian flows data\n",
    "\n",
    "The code below reads Susi's spreadsheet of Norwegian flows and then counts the number of missing values in each series. **NB:** The original file includes lots of analysis at the bottom of the sheet, below the time series themselves. I've copied the sheet and then deleted these calculations, as well as making a few other adjustments to the layout to make the data easier to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse Norwegian flows data\n",
    "nor_xls = r'\\\\niva-of5\\osl-userdata$\\JES\\Documents\\James_Work\\Staff\\Susi_S\\ECOREG\\Raw_Data\\Norway\\ECOREG discharge complete.xlsx'\n",
    "nor_df = pd.read_excel(nor_xls, \n",
    "                       sheetname='discharge ECOREG',\n",
    "                       index_col=0)\n",
    "\n",
    "# Resample to daily\n",
    "nor_df = nor_df.resample('D').mean()\n",
    "\n",
    "# Truncate before 01/10/2008 and after 30/09/2013\n",
    "nor_df = nor_df.truncate(before='2008-10-01', after='2013-09-30')\n",
    "\n",
    "# Count missing data\n",
    "nor_df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the only site with significant missing data is **Homstølvatn ndf** (S25.6). **S12.8** also has some missing values, which are identifed below as all occurring during **September 2013**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nor_df[nor_df['S12.8'].isnull()]['S12.8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the raw data, this site had a similar period of missing data during August 2013, which Susi patched using **linear interpolation**. For consistency, we can patch the gap here using the same method. Having done this, it's worth plotting the result to make sure it doesn't look too dodgy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interpolate\n",
    "nor_df['S12.8'].interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Plot\n",
    "nor_df['S12.8'].plot()\n",
    "plt.ylabel('Discharge (m3/s)')\n",
    "mpld3.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this plot it's clear that May 2013 was pretty exceptional in terms of flows (presumably a big snowmelt event?). Zooming in on August and September 2013, it's also obvious that we're doing **a lot of interpolation based on very little data**. I'll leave this in for now, but I'd be tempted to remove this from the final analysis (or at least test the effects of leaving it in versus taking it out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Confirm all missing values have been filled\n",
    "# (except at S25.6)\n",
    "nor_df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. German flows data\n",
    "\n",
    "This is a bit more complicated, because we're interested in different time periods for each of the sites. The first step is to match the site names in the German ecology spreadsheet to those in the filenames of the discharge files. This can be done in the same way as in the [previous notebook](http://nbviewer.jupyter.org/github/JamesSample/ECOREG/blob/master/ecoreg_data_exploration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input file\n",
    "in_xlsx = r'\\\\niva-of5\\osl-userdata$\\JES\\Documents\\James_Work\\Staff\\Susi_S\\ECOREG\\Raw_Data\\Germany\\sites_and_ecol.xlsx'\n",
    "\n",
    "# Read adjusted site names\n",
    "names_df = pd.read_excel(in_xlsx, sheetname='overview')\n",
    "\n",
    "# Inner join to German sites of interest on site ID\n",
    "ger_eco_df = pd.merge(names_df, ger_eco_df, how='inner', on='Gauge')\n",
    "del ger_eco_df['ID_RS']\n",
    "\n",
    "ger_eco_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now read the flows data for the sites in this table, using some slightly modified code from the [previous notebook](http://nbviewer.jupyter.org/github/JamesSample/ECOREG/blob/master/ecoreg_data_exploration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to perform parsing and resampling\n",
    "\n",
    "def read_resample_flows(file_name, \n",
    "                        site_name,\n",
    "                        skiprows=11,\n",
    "                        sep=';',\n",
    "                        decimal=',',\n",
    "                        index_col=False,\n",
    "                        dt_format='%d.%m.%Y %H:%M:%S',\n",
    "                        freq='M'):\n",
    "    \"\"\" Reads flows data and resamples to the specified frequency.\n",
    "    \n",
    "    Args:\n",
    "        file_name  File to parse\n",
    "        site_name  Name of site\n",
    "        skiprows   Number of rows to skip at start\n",
    "        sep        Column separator\n",
    "        decimal    Decimal separator\n",
    "        dt_format  String specifying date format\n",
    "        freq       Resampling frequency. 'D'=daily; 'M'=monthly; 'A'=Annual\n",
    "    \n",
    "    Returns:\n",
    "        Data frame.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_name, \n",
    "                     skiprows=skiprows,\n",
    "                     header=None,\n",
    "                     names=['Date_Time', 'Q_m3/s'],\n",
    "                     index_col=False,\n",
    "                     sep=sep,\n",
    "                     decimal=decimal) \n",
    "    \n",
    "    # Parse dates\n",
    "    df.index = pd.to_datetime(df['Date_Time'], format=dt_format)\n",
    "    del df['Date_Time']\n",
    "    \n",
    "    # Resample\n",
    "    df = df.resample(freq).mean()\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Add site name as extra column\n",
    "    df['Site'] = site_name\n",
    "    df = df[['Site', 'Date_Time', 'Q_m3/s']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process German flows data\n",
    "\n",
    "###############################################################################\n",
    "# User input\n",
    "in_fold = r'\\\\niva-of5\\osl-userdata$\\JES\\Documents\\James_Work\\Staff\\Susi_S\\ECOREG\\Raw_Data\\Germany'\n",
    "\n",
    "# Frequency for resampling\n",
    "freq = 'D'\n",
    "###############################################################################\n",
    "  \n",
    "# Get list of flow files to process\n",
    "search_path = os.path.join(in_fold, 'Flows', '*')\n",
    "file_list = glob.glob(search_path)\n",
    "\n",
    "# Read files\n",
    "df_list = []\n",
    "for file_name in file_list:\n",
    "    # Get site name\n",
    "    site_name = os.path.split(file_name)[1].split('_')[0]\n",
    "    \n",
    "    # Decide whether we need to process this site\n",
    "    if site_name in ger_eco_df['Site'].values:\n",
    "    \n",
    "        # Process differently according to file extension and file formatting\n",
    "        if site_name == 'hagen-eckesey':\n",
    "            # This site has a different date format to the other CSV files\n",
    "            df = read_resample_flows(file_name,\n",
    "                                     site_name,\n",
    "                                     skiprows=11,\n",
    "                                     sep=';',\n",
    "                                     decimal='.',\n",
    "                                     dt_format='%d.%m.%Y %H:%M',\n",
    "                                     freq=freq)        \n",
    "            df_list.append(df)\n",
    "\n",
    "        elif file_name[-3:] == 'csv':\n",
    "            # The rest of the CSV files are consistent\n",
    "            df = read_resample_flows(file_name,\n",
    "                                     site_name,\n",
    "                                     skiprows=11,\n",
    "                                     sep=';',\n",
    "                                     decimal=',',\n",
    "                                     dt_format='%d.%m.%Y %H:%M:%S',\n",
    "                                     freq=freq)        \n",
    "            df_list.append(df)\n",
    "\n",
    "        elif file_name[-3:] == 'zrx':\n",
    "            # The ZRX files are also consistent\n",
    "            df = read_resample_flows(file_name,\n",
    "                                     site_name,\n",
    "                                     skiprows=5,\n",
    "                                     sep=' ',\n",
    "                                     decimal='.',\n",
    "                                     dt_format='%Y%m%d%H%M%S',\n",
    "                                     freq=freq)        \n",
    "            df_list.append(df)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Unexpected file types found in flows data folder.')\n",
    "\n",
    "# Concatenate results\n",
    "ger_df = pd.concat(df_list, axis=0)\n",
    "\n",
    "# Pivot\n",
    "ger_df = ger_df.pivot(index='Date_Time', columns='Site', values='Q_m3/s')\n",
    "\n",
    "ger_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the 5 year period of interest for each of these sites, to get a feel for how much missing data we will have to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a list of the German sites of interest\n",
    "sites = ger_eco_df['Site'].values\n",
    "\n",
    "# Loop over sites\n",
    "nan_dict = {'nan_count':[]}\n",
    "for site in sites:\n",
    "    # Get start year and end year for this site\n",
    "    st_yr, end_yr = ger_eco_df[ger_eco_df['Site']==site][['St', 'End']].values.flatten()\n",
    "    \n",
    "    # Get series and trunctae at specified dates\n",
    "    ts = ger_df[site].truncate(before='%s-10-01' % st_yr, after='%s-09-30' % end_yr)\n",
    "    \n",
    "    # Count missing values\n",
    "    nan_cnt = ts.isnull().sum(axis=0)\n",
    "    nan_dict['nan_count'].append(nan_cnt)\n",
    "\n",
    "nan_df = pd.DataFrame(data=nan_dict, index=sites)\n",
    "\n",
    "nan_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the only German site with missing data in the periods of interest is **Meschede 3**. Let's plot this to see whether the gap represents a single large chunk, or lots of small periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get start year and end year for this site\n",
    "st_yr, end_yr = ger_eco_df[ger_eco_df['Site']=='meschede3'][['St', 'End']].values.flatten()\n",
    "print st_yr, end_yr\n",
    "\n",
    "# Get series and trunctae at specified dates\n",
    "ts = ger_df['meschede3'].truncate(before='%s-10-01' % st_yr, after='%s-09-30' % end_yr)\n",
    "\n",
    "# Plot\n",
    "ts.plot()\n",
    "mpld3.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note about this series is that it looks unnatural - either there are problems with the flow gauging here or this is a heavily modified flow regime. **It might be worth checking the other series to identify these kinds of patterns?**\n",
    "\n",
    "Regarding missing data, it appears there are a few small data gaps in the middle of the series, but most of the NaNs are concentrated at the end, between July and September 2012. It is therefore worth applying **linear interpolation** to fill the small data gaps, but there's nothing we can sensibly do about the large gap at the end of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get start time of big data gap at the end of the series\n",
    "gap_st = ger_df['meschede3'].last_valid_index()\n",
    "\n",
    "# Interpolate\n",
    "ger_df['meschede3'][:gap_st] = ger_df['meschede3'][:gap_st].interpolate(method='linear')\n",
    "\n",
    "# Get series and trunctae at specified dates\n",
    "ts = ger_df['meschede3'].truncate(before='%s-10-01' % st_yr, after='%s-09-30' % end_yr)\n",
    "\n",
    "print ts.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpolation algorithm has filled-in values for just three time steps - there are still 82 missing values left at the end of the series.\n",
    "\n",
    "### 2.3. Summary of discharge data availability\n",
    "\n",
    " * Overall, there are **28 German sites** and **40 Norwegain sites** for which we have broadly compatible flow, chemistry and ecology data. <br><br>\n",
    " \n",
    " * For the 5 year period prior to ecological surveying and water chemistry sampling, we have complete daily discharge records for **66 of the 68 sites**. <br><br>\n",
    " \n",
    " * At **Homstølvatn ndf** (site 25.6) in Norway, data are missing for the whole of 2009. This will make it difficult to estimate the IHA parameters for the 2009 and 2010 water years, but we can still estimate values based on water years **2011 to 2013 inclusive**. <br><br>\n",
    " \n",
    " * At site **Meschede 3** in Germany, data are missing from July to September 2012. This will make it difficult to estimate the IHA parameters for the 2012 water year, but we can still calculate values based on water years **2008 to 2011 inclusive**.\n",
    " \n",
    "## 3. Calculation of the IHA parameters\n",
    "\n",
    "The following code is based on work in the [previous notebook](http://nbviewer.jupyter.org/github/JamesSample/ECOREG/blob/master/ecoreg_data_exploration.ipynb) and uses [R's IHA package](http://deq2.bse.vt.edu/sifnwiki/index.php/R_iha) to calculate the IHA parameters for each time series. We start by defining a fucntion to perform the calculation, linking Python to R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_iha(df):\n",
    "    \"\"\" Processes the first five groups of IHA indicators.\n",
    "    \n",
    "    Args:\n",
    "        df  Pandas data frame with DAILY resolution consisting\n",
    "            of a single column entitled 'site_name' and a date-time\n",
    "            index.\n",
    "    \n",
    "    Returns:\n",
    "        Pandas data frame of IHA parameters calculated from the\n",
    "        IHA R package.\n",
    "    \"\"\"\n",
    "    # Set up connection to R. This all seems unnecessarily complicated!\n",
    "    import rpy2.interactive as r\n",
    "    import rpy2.interactive.packages\n",
    "    from rpy2.robjects.packages import importr\n",
    "    import pandas.rpy.common as com\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    pandas2ri.activate()\n",
    "    \n",
    "    # Circular mean function from scipy (see above)\n",
    "    from scipy.stats import circmean\n",
    "\n",
    "    # Load necessary R packages\n",
    "    importr('zoo', lib_loc=\"//niva-of5/osl-userdata$/JES/Documents/R/win-library/3.2\")   \n",
    "    importr('IHA', lib_loc=\"//niva-of5/osl-userdata$/JES/Documents/R/win-library/3.2\")\n",
    "    \n",
    "    # Import R packages into interactive session\n",
    "    zoo = r.packages.importr('zoo')\n",
    "    iha = r.packages.importr('IHA')\n",
    "    \n",
    "    # Get path to package methods\n",
    "    rlib = r.packages.packages\n",
    "\n",
    "    # Convert df to 2 columns ['Dates', 'Flows']\n",
    "    df2 = df.reset_index()\n",
    "\n",
    "    # Convert Pandas df to R \n",
    "    ts = rlib.zoo.read_zoo(df2, format=\"%Y-%m-%d\")\n",
    "    \n",
    "    # Processing for Group 1\n",
    "    # Calculate group 1 stats.\n",
    "    rg1 = rlib.IHA.group1(ts)\n",
    "\n",
    "    # Convert back to Python\n",
    "    grp1 = com.convert_robj(rg1)\n",
    "\n",
    "    # Get stats\n",
    "    grp1 = grp1.describe().T\n",
    "\n",
    "    # Coefficient of dispersion\n",
    "    grp1['CoD'] = (grp1['75%'] - grp1['25%']) / grp1['50%']\n",
    "\n",
    "    # Format grp 1 df\n",
    "    grp1.index.name = 'Indicator'\n",
    "    grp1.reset_index(inplace=True)\n",
    "    grp1['Group'] = 1\n",
    "    grp1.index = [grp1['Group'], grp1['Indicator']]\n",
    "    grp1 = grp1[['50%', 'CoD']]\n",
    "    \n",
    "    # Processing for Group 2\n",
    "    # Calculate group 2 stats.\n",
    "    rg2 = rlib.IHA.group2(ts)\n",
    "\n",
    "    # Convert back to Python\n",
    "    grp2 = com.convert_robj(rg2)\n",
    "\n",
    "    # Get stats\n",
    "    grp2 = grp2.describe().T\n",
    "\n",
    "    # Coefficient of dispersion\n",
    "    grp2['CoD'] = (grp2['75%'] - grp2['25%']) / grp2['50%']\n",
    "\n",
    "    # Format grp 2 df\n",
    "    grp2.index.name = 'Indicator'\n",
    "    grp2.reset_index(inplace=True)\n",
    "    grp2['Group'] = 2\n",
    "    grp2 = grp2[grp2['Indicator'] != 'year']\n",
    "    grp2.index = [grp2['Group'], grp2['Indicator']]\n",
    "    grp2 = grp2[['50%', 'CoD']]\n",
    "\n",
    "    # Processing for Group 3\n",
    "    # Calculate group 3 stats.\n",
    "    rg3 = rlib.IHA.group3(ts)\n",
    "\n",
    "    # Convert back to Python\n",
    "    grp3 = com.convert_robj(rg3)\n",
    "\n",
    "    # Get stats using circular mean and assuming 366 days per year (as in IHA)\n",
    "    c_av = circmean(grp3, high=366, low=0, axis=0)\n",
    "    \n",
    "    # We won't include a CoD for this stat.\n",
    "    # Build df to store this info\n",
    "    grp3 = pd.DataFrame(data=[c_av, [pd.np.nan, pd.np.nan]], \n",
    "                        columns=['Min', 'Max'],\n",
    "                        index=['50%', 'CoD']).T\n",
    "\n",
    "    # Format grp 2 df\n",
    "    grp3.index.name = 'Indicator'\n",
    "    grp3.reset_index(inplace=True)\n",
    "    grp3['Group'] = 3\n",
    "    grp3.index = [grp3['Group'], grp3['Indicator']]\n",
    "    grp3 = grp3[['50%', 'CoD']]\n",
    "\n",
    "    # Processing for Group 4\n",
    "    # Calculate group 4 stats.\n",
    "    rg4 = rlib.IHA.group4(ts)\n",
    "\n",
    "    # Convert back to Python\n",
    "    grp4 = com.convert_robj(rg4)\n",
    "\n",
    "    # Get stats\n",
    "    grp4 = grp4.describe().T\n",
    "\n",
    "    # Coefficient of dispersion\n",
    "    grp4['CoD'] = (grp4['75%'] - grp4['25%']) / grp4['50%']\n",
    "\n",
    "    # Format grp 4 df\n",
    "    grp4.index.name = 'Indicator'\n",
    "    grp4.reset_index(inplace=True)\n",
    "    grp4['Group'] = 4\n",
    "    grp4.index = [grp4['Group'], grp4['Indicator']]\n",
    "    grp4 = grp4[['50%', 'CoD']]  \n",
    "\n",
    "    # Processing for Group 5\n",
    "    # Calculate group 5 stats.\n",
    "    rg5 = rlib.IHA.group5(ts)\n",
    "\n",
    "    # Convert back to Python\n",
    "    grp5 = com.convert_robj(rg5)\n",
    "\n",
    "    # Get stats\n",
    "    grp5 = grp5.describe().T\n",
    "\n",
    "    # Coefficient of dispersion\n",
    "    grp5['CoD'] = (grp5['75%'] - grp5['25%']) / grp5['50%']\n",
    "\n",
    "    # Format grp 4 df\n",
    "    grp5.index.name = 'Indicator'\n",
    "    grp5.reset_index(inplace=True)\n",
    "    grp5['Group'] = 5\n",
    "    grp5.index = [grp5['Group'], grp5['Indicator']]\n",
    "    grp5 = grp5[['50%', 'CoD']]  \n",
    "\n",
    "    # Combine results\n",
    "    iha_res = pd.concat([grp1, grp2, grp3, grp4, grp5], axis=0)\n",
    "    \n",
    "    # Rename 50% col (because not all values are actually medians)\n",
    "    # Also add heirarchical index for site_name\n",
    "    iha_res.columns = [[df.columns[0], df.columns[0]],['Cent_Est', 'CoD']]\n",
    "   \n",
    "    return iha_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop over the flows data for each site in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tables contains the 33 IHA parameters for the 68 sites under consideration.\n",
    "\n",
    "## 4. Basic data exploration\n",
    "\n",
    "Before linking the IHA flow parameters to any other data, it's worth trying a few basic plots to see if these numbers make sense.\n",
    "\n",
    " * **To do:** Process one of the sites manually using Excel and R to check that the output is the same as from my code.\n",
    "\n",
    "### 4.1. Seasonality\n",
    "\n",
    "To begin with, let's see if there are any obvious differences in seasonality between Germany and Norway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = iha_df.copy()\n",
    "\n",
    "# Add column specifying country\n",
    "df['Country'] = df.index.map(lambda x: 'Norway' if x[0]=='S' else 'Germany')\n",
    "\n",
    "# Extract just monthly medians\n",
    "months = ['October', 'November', 'December', 'January', 'February',\n",
    "          'March', 'April', 'May', 'June', 'July', 'August', 'September']\n",
    "mon_df = df[months+['Country',]]\n",
    "\n",
    "# Get the median for each country in each month\n",
    "meds = mon_df.groupby('Country').median().T\n",
    "\n",
    "# Plot\n",
    "meds.plot()\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Median flow ($m^3/s$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a pretty big contrast here. Perhaps unsurprisingly, the Norwegian sites typically have a huge meltwater pulse during the spring, and flows then decline steadily throughout the summer and autumn to a minimum at the end of the winter. In contrast, the German sites are characterised by much lower variability overall, and by flows at their highest during the winter and their lowest in the summer. This seems sensible.\n",
    "\n",
    "We can also get a feel for the variability between sites by creating boxplots for each month. Note that I've **log-transformed** the data here as otherwise the Norwegian snowmelt pulses completely dominate the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Restructure df for use with 'factorplot'\n",
    "mon_df = pd.melt(mon_df, id_vars=['Country'], value_vars=months, value_name='Flow')\n",
    "\n",
    "# Log-transform flows\n",
    "mon_df['Flow'] = np.log10(mon_df['Flow'])\n",
    "\n",
    "# Plot\n",
    "with sn.plotting_context('poster'):\n",
    "    g = sn.factorplot('Indicator', 'Flow', 'Country', data=mon_df, kind='box', size=10, aspect=2)\n",
    "    g.set_axis_labels('Month', '$log_{10}(Flow \\; in \\; m^3/s)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Julian day of min and max flow\n",
    "\n",
    "Based on the plots above, we might also expect the IHA Group 3 parameters (describing the timing of minimum and maximum flows) to be distinct between Norway and Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get data of interest\n",
    "cols = ['Min', 'Max', 'Country']\n",
    "grp3_df = df[cols]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,6))\n",
    "\n",
    "nor_min = grp3_df[grp3_df['Country']=='Norway']['Min']\n",
    "nor_max = grp3_df[grp3_df['Country']=='Norway']['Max']\n",
    "ger_min = grp3_df[grp3_df['Country']=='Germany']['Min']\n",
    "ger_max = grp3_df[grp3_df['Country']=='Germany']['Max']\n",
    "\n",
    "# Mins\n",
    "axes[0].hist([nor_min, ger_min], 50, \n",
    "             stacked=True, normed=False, \n",
    "             label=['Norway', 'Germany'])\n",
    "axes[0].set_title('Julian day of minimum flow')\n",
    "axes[0].set_xlabel('Day of year')\n",
    "axes[0].set_ylabel('Number of sites')\n",
    "\n",
    "# Maxes\n",
    "axes[1].hist([nor_max, ger_max], 50, \n",
    "             stacked=True, normed=False, \n",
    "             label=['Norway', 'Germany'])\n",
    "axes[1].set_title('Julian day of maximum flow')\n",
    "axes[1].set_xlabel('Day of year')\n",
    "axes[1].set_ylabel('Number of sites')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These histograms are clearly **multimodal** and consistent with the seasonality plots above: in Norway, the vast majority of sites have their minimum flows during the winter, from around Julian day 330 to Julian day 100 (roughly December to March); in Germany, minima usually occur between Julian days 175 and 275 (roughly July to September). For maxima, the opposite is true: in Norway most maximum flows occur between Julian days 150 and 175 (roughly June), whereas in Germany the maxima are typicaly between days 350 and 50 (mid-December to late February).\n",
    "\n",
    "### 4.3. Principal component analysis\n",
    "\n",
    "Now that the Norwegian data have been incorporated, it could be interesting to repeat the PCA analysis from the [previous notebook](http://nbviewer.jupyter.org/github/JamesSample/ECOREG/blob/master/ecoreg_data_exploration.ipynb) to see whether the Norwegian and German sites are **distinguishable within the IHA parameter space**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Delete the \"Zero flows days\" indicator, as it's always 0\n",
    "del iha_df['Zero flow days']\n",
    "\n",
    "# Standardise the feature data\n",
    "feat_std = StandardScaler().fit_transform(iha_df)\n",
    "\n",
    "# Setup PCA. Initially, choose to keep ALL components\n",
    "pca = PCA()\n",
    "\n",
    "# Fit model\n",
    "pca.fit(feat_std)\n",
    "\n",
    "# Get explained variances (in %)\n",
    "var_exp = 100*pca.explained_variance_ratio_\n",
    "cum_exp = np.cumsum(var_exp)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "plt.bar(range(1, len(var_exp)+1), var_exp, align='center', label='Individual components')\n",
    "plt.plot(range(1, len(cum_exp)+1), cum_exp, 'r-o', label='Cumulative')\n",
    "plt.xlabel('Principal component')\n",
    "plt.ylabel('Variance explained (%)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print 'Variance explained by first 5 PCs (%):\\n'\n",
    "print var_exp[:5]\n",
    "print '\\nTotal: %.2f%%' % var_exp[:5].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first two PCs explain roughly **72%** of the variance and the first 3 about **80%**. The plot below shows the projection of the data into the space defined by the first 3 PCs. Norwegian sites are **red**; German sites are **blue**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Refit the PCA, this time specifying 3 components\n",
    "# and transforming the result\n",
    "feat_reduced = PCA(n_components=3).fit_transform(feat_std)\n",
    "\n",
    "# Plot\n",
    "colors = {'Norway':'red', 'Germany':'blue'}\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=135)\n",
    "scat = ax.scatter(feat_reduced[:, 0], feat_reduced[:, 1], feat_reduced[:, 2],\n",
    "                  c=df['Country'].apply(lambda x: colors[x]))\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests the majority of the sites lie approximately on a plane embedded in the space defined by the first three PCs. The main exception to this is that one of the Norwegian sites lies well away from all the others. Let's try a projection into 2D as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpld3 import plugins\n",
    "\n",
    "# Refit the PCA, this time specifying 2 components\n",
    "# and transforming the result\n",
    "feat_reduced = PCA(n_components=2).fit_transform(feat_std)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(feat_reduced[:, 0], feat_reduced[:, 1],\n",
    "                     c=df['Country'].apply(lambda x: colors[x]))\n",
    "tooltip = mpld3.plugins.PointLabelTooltip(scatter, labels=list(iha_df.index))\n",
    "mpld3.plugins.connect(fig, tooltip)\n",
    "ax.set_title(\"First two PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "mpld3.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, it is possible to identify individual sites by hovering the mouse over points on the figure above. \n",
    "\n",
    "Although there is no clear distinction between the German and Norwegian stations, it is notable that all the German sites are associated with negative values of the second PC, whereas most of the Norwegain sites have positive values. In many cases, there is a clear separation in these two dimensions between the Norwegian sites (in red) and the German ones (in blue).\n",
    "\n",
    "Another point worth noting is that the first PC here is very heavily influenced by a single outlier (Norwegian site S2.611). One drawback of standard PCA is that it is very sensitive to such outliers, and in this case the direction of the first eigenvector looks to be determined largely by this single point. If site S2.611 was removed from the analysis, the first PC would actually have less variance than the second, so this plot could be misleading: given that these points are projections from a 32 dimensional space, it is possible that a different affine transformation could achieve better separation of the other points if S2.611 was removed. Let's test this.\n",
    "\n",
    "**NB:** It might be worth trying **[Sparse PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html)** here. This introduces an **L1 regularisation penalty**, which may make the results more robust to outliers such as S2.611."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete S2.611\n",
    "df2 = iha_df.copy()\n",
    "df2 = df2[df2.index != 'S2.611']\n",
    "\n",
    "# Standardise the feature data\n",
    "feat_std = StandardScaler().fit_transform(df2)\n",
    "\n",
    "# Setup PCA. Initially, choose to keep ALL components\n",
    "pca = PCA()\n",
    "\n",
    "# Refit the PCA, this time specifying 2 components\n",
    "# and transforming the result\n",
    "feat_reduced = PCA(n_components=2).fit_transform(feat_std)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(feat_reduced[:, 0], feat_reduced[:, 1],\n",
    "                     c=df['Country'].apply(lambda x: colors[x]))\n",
    "tooltip = mpld3.plugins.PointLabelTooltip(scatter, labels=list(iha_df.index))\n",
    "mpld3.plugins.connect(fig, tooltip)\n",
    "ax.set_title(\"First two PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "mpld3.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a slightly different view of the data, although the conclusions are largely the same. Many of the Norwegian sites are hydromorphologically distinct from the German ones, to the extent that drawing a single straight line on the plot above would do a fairly good job of separating the points into two classes. There are, however, a handful of Norwegain sites (approximately 8 looking at the plot above) that are essentially indistinguishable from the German ones. A brief exploration of this \"confusion\" (e.g. comparing the plot above with the histograms showing the Julian day of maximum and minimum flows) suggests that, unfortunately, these points do not have a straightforward explanation in terms of individual IHA parameters. It might be interesting to look at the geographical location of these sites though?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
